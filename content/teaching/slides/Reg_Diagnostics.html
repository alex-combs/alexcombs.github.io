<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>PADP 7120 Data Applications in PA</title>
    <meta charset="utf-8" />
    <meta name="author" content="Alex Combs" />
    <script src="Reg_Diagnostics_files/header-attrs-2.11/header-attrs.js"></script>
    <link rel="stylesheet" href="mypres.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# PADP 7120 Data Applications in PA
## Regression Diagnostics
### Alex Combs
### UGA | SPIA | PADP
### Last updated: April 24, 2022

---

# Outline



- Are my regression results statistically valid?
  - Classic assumptions of regression (LINE)
  - Multicollinearity
  - Outliers, high-leverage, and high-influence

- Goal is to have sufficient knowledge to ask good questions and explain consequences if an analyst did not check assumptions

---
class: inverse, center, middle

# Classical assumptions of linear regression

---
# Regression table

`$$TrumpVote = \beta_0 + \beta_1PctWhite + \beta_2 PctWhitePov + \epsilon$$` 




|term      | estimate| std_error| statistic| p_value| lower_ci| upper_ci|
|:---------|--------:|---------:|---------:|-------:|--------:|--------:|
|intercept |    10.93|      6.53|      1.68|     0.1|    -2.19|    24.05|
|white     |     0.26|      0.08|      3.24|     0.0|     0.10|     0.43|
|white_pov |     2.18|      0.54|      4.01|     0.0|     1.09|     3.27|

- We have finally covered all the information in this table. 

- But the information could be wrong if regression assumptions are violated.

---
# Concerns

- Our data or regression model choices may cause **LLN** or **CLT** to fail.

- Biased estimates
  - Systematically higher/lower estimates than the parameter

- Invalid hypothesis test
  - Inflated chance of false positives or false negatives

- Wider confidence intervals than necessary (i.e., less precision)
  - Undesirable but tolerable

---
# It's all about residuals



&lt;img src="Reg_Diagnostics_files/figure-html/unnamed-chunk-5-1.png" style="display: block; margin: auto;" /&gt;

---
# Classical regression assumptions

- LINE

- **L**inear relationship between `\(x\)`s and `\(y\)` (or nonlinear relationship included appropriately)

--

- **I**ndependent residuals (independent observations; random sampling)

--

- **N**ormality of residuals

--

- **E**qual variance in residuals

---
# Residual vs. Fitted Plot (RVF)

.pull-left[
&lt;img src="Reg_Diagnostics_files/figure-html/unnamed-chunk-6-1.png" style="display: block; margin: auto;" /&gt;
]

.pull-right[
&lt;img src="Reg_Diagnostics_files/figure-html/unnamed-chunk-7-1.png" style="display: block; margin: auto;" /&gt;
]

- Assumptions **L**, **N**, and **E** can be observed using an RVF plot (on right).

- We want to see no *obvious* pattern in the RVF plot points

---
# Linear assumption

&lt;img src="lectures_files/rvfp-linear.png" width="1260" style="display: block; margin: auto;" /&gt;

- Obvious pattern in RVF on the right

- Violation of **L**

---
# Linear assumption

&lt;img src="lectures_files/rvfp-linear2.png" width="1272" style="display: block; margin: auto;" /&gt;

- Changing the model to quadratic has improved the RVF

---
# Normally distributed residuals

&lt;img src="lectures_files/norm-residuals.png" width="1109" style="display: block; margin: auto;" /&gt;

---
# Normally distributed residuals

&lt;img src="lectures_files/rvfp-normality.png" width="1251" style="display: block; margin: auto;" /&gt;

- Pattern on right shows a highly skewed distribution of residuals

- Violation of **N**

---
# Equal variance in residuals

&lt;img src="lectures_files/homo-hetero.png" width="1264" style="display: block; margin: auto;" /&gt;

---
# Equal variance in residuals

&lt;img src="lectures_files/homoskedasticity.png" width="1251" style="display: block; margin: auto;" /&gt;

- Spread of the distribution is constant along the regression line

---
# Equal variance in residuals

&lt;img src="lectures_files/heteroskedasticity.png" width="1223" style="display: block; margin: auto;" /&gt;

- Spread of distribution clearly changing

- Violation of **E**

---
# Independent residuals/observations

- Cannot directly observe violation of **I**

- If there are signs that **L** or **N** have been violated, could be violation of **I**

--

- Evaluating **I** requires an understanding of how sample was collected

- Independence is hard to achieve unless sample was truly random

---
# Consequences

- Violation of **L** or **N** or **I**
  - Biased estimates

- Violation of **E**
  - Biased standard errors, thus biased confidence intervals
  - Invalid hypothesis testing

---
# Solutions

- Violation of **L** or **N**
  - Modify the regression model

- Violation of **E**
  - Log transform the outcome variable
  - Statistical correction outside scope of class (robust standard errors)

- Violation of **I**
  - Obtain a better sample
  - Control for the variables related to non-randomness
  - Statistical correction outside scope of class (clustered standard errors)

---
class: inverse, center, middle

# Multicollinearity

---
# Multicollinearity

- When two explanatory variables are strongly correlated with each other

- Since regression computes the association between an `\(x\)` and `\(y\)`, **controlling for all other `\(x\)`s**, multicollinearity can mask statistically significant associations between those `\(x\)`s and the `\(y\)`.

--

- Multicollinearity can cause false negatives

---
# Multicollinearity

- Detection
  - If two variables have a correlation stronger than +/- 0.8, multicollinearity might be a problem
  - Variance Inflation Factor (VIF) &gt; 10 is another method

- Solutions
  - Combine the collinear variables into a single index variable
  - If one variable is really important to your RQ, drop the other. Be careful not to introduce OVB!

---
class: inverse, center, middle

# Unusual and influential data

---
# Unusual and influential data

- Regression outlier
  - An observation with a large residual

--

- High-leverage
  - An observation with a large deviation from the explanatory variable's mean

--

- High-influence
  - A regression outlier with high leverage

--

- An influential observation is an observation that, if removed, meaningfully changes regression results

---
class: inverse, center, middle

# Regression diagnostics in R

---
# Revisiting Trump Support

`$$TrumpVote = \beta_0 + \beta_1PctWhite + \beta_2 PctWhitePov + \epsilon$$` 


|term      | estimate| std_error| statistic| p_value| lower_ci| upper_ci|
|:---------|--------:|---------:|---------:|-------:|--------:|--------:|
|intercept |    10.93|      6.53|      1.68|     0.1|    -2.19|    24.05|
|white     |     0.26|      0.08|      3.24|     0.0|     0.10|     0.43|
|white_pov |     2.18|      0.54|      4.01|     0.0|     1.09|     3.27|

- We got our initial results. 

- Now we should check assumptions.

---
# In R

- After saving a regression model...


```r
trump_mod &lt;- lm(share_vote_trump ~ white + white_pov,
                data = state_trump)
```

- Run the following, which generates four diagnostic plots


```r
plot(trump_mod)
```

- Each plot targets a specific assumption or issue

---
# RVF Plot

&lt;img src="Reg_Diagnostics_files/figure-html/unnamed-chunk-18-1.png" style="display: block; margin: auto;" /&gt;

- We want to see no obvious pattern and a relatively straight red line along 0

- Especially useful for evaluating **L**

---
# Normal Q-Q

&lt;img src="Reg_Diagnostics_files/figure-html/unnamed-chunk-19-1.png" style="display: block; margin: auto;" /&gt;

- We want to see points fall approximately along the dotted line

- If not, suggests **N** may be violated

---
# Scale-Location

&lt;img src="Reg_Diagnostics_files/figure-html/unnamed-chunk-20-1.png" style="display: block; margin: auto;" /&gt;

- We want to see a straight red line

- If not, evidence **E** may be violated

---
# Residuals vs. Leverage

&lt;img src="Reg_Diagnostics_files/figure-html/unnamed-chunk-21-1.png" style="display: block; margin: auto;" /&gt;

- Useful for identifying high-influence observations

- Observations that cross the dotted Cook's distance may be a problem

---
# Cook's distance

- Cook's distance is the most common measure for identifying high-influence observations

- One rule of thumb is that an observation has high influence if Cook's distance exceeds...

`$$4/(n-p-1)$$`

- Where `\(n\)` is the number of observations and `\(p\)` the number of explanatory variables in our model

- Even if an observation does not meet this threshold, it is worth considering whether an observation is somehow different from other observations

---
# Influencer?

- Model has 51 observations and 2 explanatory variables. 

- Threshold is:


```r
4/(51-2-1)
```

```
## [1] 0.08333333
```

---
# Cook's distance


```r
plot(trump_mod, 4)
```

&lt;img src="Reg_Diagnostics_files/figure-html/unnamed-chunk-23-1.png" style="display: block; margin: auto;" /&gt;

- Observations 9, 18, 20, and 46 may be worth investigating

---
# Precise Cook's distance


```r
library(broom)
augment(trump_mod) %&gt;% 
  arrange(desc(.cooksd))
```


| share_vote_trump| .fitted|  .resid|  .hat| .sigma| .cooksd| .std.resid|
|----------------:|-------:|-------:|-----:|------:|-------:|----------:|
|                4|  29.418| -25.418| 0.146|  8.326|   0.516|     -3.008|
|               33|  57.541| -24.541| 0.068|  8.466|   0.187|     -2.779|
|               45|  61.112| -16.112| 0.070|  8.915|   0.084|     -1.827|
|               63|  70.435|  -7.435| 0.221|  9.160|   0.080|     -0.921|
|               70|  52.986|  17.014| 0.042|  8.888|   0.053|      1.901|
|               53|  39.989|  13.011| 0.064|  9.032|   0.049|      1.470|
|               53|  39.320|  13.680| 0.054|  9.012|   0.045|      1.538|
|               60|  47.041|  12.959| 0.052|  9.036|   0.039|      1.455|
|               62|  50.277|  11.723| 0.046|  9.075|   0.028|      1.312|
|               33|  40.851|  -7.851| 0.087|  9.164|   0.026|     -0.898|
|               65|  49.891|  15.109| 0.024|  8.969|   0.023|      1.672|
|               57|  47.305|   9.695| 0.054|  9.127|   0.023|      1.090|
|               64|  52.986|  11.014| 0.042|  9.095|   0.022|      1.231|
|               34|  47.639| -13.639| 0.028|  9.019|   0.022|     -1.512|
|               63|  54.253|   8.747| 0.051|  9.149|   0.017|      0.982|
|               37|  48.044| -11.044| 0.033|  9.096|   0.017|     -1.228|
|               41|  52.265| -11.265| 0.023|  9.092|   0.012|     -1.246|
|               38|  48.765| -10.765| 0.020|  9.105|   0.010|     -1.189|
|               58|  52.407|   5.593| 0.065|  9.204|   0.009|      0.632|
|               51|  44.280|   6.720| 0.040|  9.188|   0.008|      0.750|
|               50|  44.017|   5.983| 0.043|  9.199|   0.007|      0.669|
|               40|  47.375|  -7.375| 0.027|  9.178|   0.006|     -0.817|
|               69|  66.002|   2.998| 0.116|  9.231|   0.005|      0.349|
|               55|  47.446|   7.554| 0.021|  9.175|   0.005|      0.835|
|               40|  42.768|  -2.768| 0.103|  9.233|   0.004|     -0.320|
|               46|  41.572|   4.428| 0.045|  9.219|   0.004|      0.496|
|               60|  56.627|   3.373| 0.046|  9.229|   0.002|      0.377|
|               39|  42.820|  -3.820| 0.035|  9.225|   0.002|     -0.425|
|               58|  56.241|   1.759| 0.131|  9.238|   0.002|      0.206|
|               61|  58.545|   2.455| 0.068|  9.235|   0.002|      0.278|
|               30|  31.213|  -1.213| 0.199|  9.240|   0.002|     -0.148|
|               35|  37.209|  -2.209| 0.065|  9.236|   0.001|     -0.250|
|               47|  49.750|  -2.750| 0.041|  9.233|   0.001|     -0.307|
|               45|  43.470|   1.530| 0.112|  9.239|   0.001|      0.178|
|               48|  51.139|  -3.139| 0.027|  9.231|   0.001|     -0.348|
|               42|  45.001|  -3.001| 0.025|  9.232|   0.001|     -0.332|
|               45|  42.556|   2.444| 0.036|  9.235|   0.001|      0.272|
|               48|  50.612|  -2.612| 0.025|  9.234|   0.001|     -0.289|
|               57|  54.710|   2.290| 0.031|  9.236|   0.001|      0.254|
|               59|  57.084|   1.916| 0.042|  9.238|   0.001|      0.214|
|               47|  48.025|  -1.025| 0.116|  9.241|   0.001|     -0.119|
|               41|  42.486|  -1.486| 0.057|  9.240|   0.001|     -0.167|
|               51|  49.100|   1.900| 0.027|  9.238|   0.000|      0.211|
|               57|  58.210|  -1.210| 0.049|  9.241|   0.000|     -0.136|
|               52|  53.584|  -1.584| 0.028|  9.239|   0.000|     -0.176|
|               49|  50.612|  -1.612| 0.025|  9.239|   0.000|     -0.178|
|               42|  40.973|   1.027| 0.041|  9.241|   0.000|      0.115|
|               52|  52.986|  -0.986| 0.042|  9.241|   0.000|     -0.110|
|               57|  56.486|   0.514| 0.054|  9.242|   0.000|      0.058|
|               44|  44.403|  -0.403| 0.037|  9.242|   0.000|     -0.045|
|               49|  49.170|  -0.170| 0.055|  9.242|   0.000|     -0.019|

---
# Identifying an observation

- Which observations are rows 9, 18, 20, and 46? Can use subsetting.


```r
state_trump[c(9,18,20,46), ]
```


|state                | med_inc|   hs| white| white_pov| share_vote_trump|urban |
|:--------------------|-------:|----:|-----:|---------:|----------------:|:-----|
|District of Columbia |   68277| 87.1|    37|         4|                4|high  |
|Kentucky             |   42786| 81.7|    85|        17|               63|low   |
|Maine                |   51710| 90.2|    91|        12|               45|low   |
|Vermont              |   60708| 91.0|    94|        10|               33|low   |

- D.C. probably warrants exclusion. Oters probably do not.

---
# Influencer?


```r
state_trump_nodc &lt;- state_trump %&gt;%
  filter(state != "District of Columbia")

trump_mod_nodc &lt;- lm(share_vote_trump ~ white + white_pov,
                data = state_trump_nodc)
```


```r
get_regression_table(trump_mod_nodc)
```


|term      | estimate| std_error| statistic| p_value| lower_ci| upper_ci|
|:---------|--------:|---------:|---------:|-------:|--------:|--------:|
|intercept |    18.86|      6.41|      2.94|    0.00|     5.97|    31.75|
|white     |     0.21|      0.08|      2.79|    0.01|     0.06|     0.36|
|white_pov |     1.77|      0.51|      3.47|    0.00|     0.74|     2.80|

- Need to compare this to the initial results

---
# Results including DC


|term      | estimate| std_error| statistic| p_value| lower_ci| upper_ci|
|:---------|--------:|---------:|---------:|-------:|--------:|--------:|
|intercept |    10.93|      6.53|      1.68|     0.1|    -2.19|    24.05|
|white     |     0.26|      0.08|      3.24|     0.0|     0.10|     0.43|
|white_pov |     2.18|      0.54|      4.01|     0.0|     1.09|     3.27|

- Estimates changed.

- Meaningfully? Probably not. Nothing changed with respect to the sign and statistical significance of each estimate. The confidence intervals mostly overlap.

---
# Comparing regression lines

.pull-left[
&lt;img src="Reg_Diagnostics_files/figure-html/unnamed-chunk-32-1.png" style="display: block; margin: auto;" /&gt;
]

.pull-right[
&lt;img src="Reg_Diagnostics_files/figure-html/unnamed-chunk-33-1.png" style="display: block; margin: auto;" /&gt;
]

- D.C. (on left) pulls the left of regression line down; forces slope to be greater

---
# Statistical test on assumptions


```r
library(gvlma)
gvlma(trump_mod)
```

&lt;img src="lectures_files/gvlma1.png" width="1064" style="display: block; margin: auto;" /&gt;

- Global Stat: Overall test of your model
- Skewness: Primary test of **N**; secondary test for influence
- Kurtosis: Primary test for influence; secondary test of **N**
- Link Function: Test of **L**
- Heteroskedasticity: Test of **E**

---
# Statistical test on assumptions


```r
gvlma(trump_mod_nodc)
```
&lt;img src="lectures_files/gvlma2.png" width="1056" style="display: block; margin: auto;" /&gt;

- Exclusion of DC makes a difference

---
# Checking multicollinearity


```r
library(car)
```

```r
vif(trump_mod_nodc)
```

```
##     white white_pov 
##  1.043766  1.043766
```

- Nothing greater than 10; no multicollinearity
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="macros.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>

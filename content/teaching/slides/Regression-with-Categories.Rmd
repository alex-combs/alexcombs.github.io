---
title: "PADP 7120 Data Applications in PA"
subtitle: "Regression with Categorical Variables"
author: "Alex Combs"
institute: "UGA | SPIA | PADP"
date: "Last updated: `r format(Sys.time(), '%B %d, %Y')`"
output:
  xaringan::moon_reader:
    css: 'mypres.css'
    nature:
      beforeInit: "macros.js"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
# Outline

```{r, include=FALSE}
library(tidyverse)
library(moderndive)
library(knitr)
library(carData)
options(scipen = 999)
```

- Add categorical variables as explanatory variables

- Interact numerical and categorical variables

- Use a binary categorical variable as an outcome

---
# Adding to our toolbox

- So far, we have covered simple and multiple regression using quantitative/numerical variables

.font120[$$y=\beta_0+\beta_1x_1+\beta_2x_2+\dots+\beta_kx_k+\epsilon$$]

- Any of the explanatory variables could be categorical

- The outcome could be categorical as well

---
# Example

- Suppose HR wants to examine if male professors are paid differently than female professors

```{r}
glimpse(Salaries)
```

- discipline coded as "A" for theoretical and "B" for applied

---
# Example

.pull-left[
```{r, eval=FALSE}
ggplot(Salaries, 
       aes(x = yrs.service, 
           y = salary)) +
  geom_point(size = 4) +
  theme_minimal() +
  theme(
    text = element_text(size = 20))
```
]

.pull-right[
```{r, echo=FALSE, out.width='99%'}
ggplot(Salaries, aes(x = yrs.service, y = salary)) +
  geom_point(size = 4) +
  theme_minimal() +
  theme(text = element_text(size = 20))
```
]

---
# Example

.pull-left[
```{r, eval=FALSE}
ggplot(Salaries, 
       aes(x = yrs.service, 
           y = salary, 
           color = sex)) + #<<
  geom_point(size = 4,
             alpha = 0.5) +
  theme_minimal() +
  theme(text = element_text(size = 20))
```
]

.pull-right[
```{r, echo=FALSE, out.width='99%'}
ggplot(Salaries, aes(x = yrs.service, y = salary, color = sex)) +
  geom_point(size = 4,
             alpha = 0.5) +
  theme_minimal() +
  theme(text = element_text(size = 20))
```
]

---
# Visualizing Parallel Slopes

```{r, echo=FALSE, fig.width=8, fig.align='center'}
ggplot(Salaries, aes(x = yrs.service, y = salary, color = sex)) +
  geom_point(size = 4, alpha = 0.5) +
  geom_abline(slope = 747, intercept = 92357, color = 'red', size = 2) +
  geom_abline(slope = 747, intercept = 101429, color = 'blue', size = 2) +
  theme_minimal() +
  theme(text = element_text(size = 20))
```

---
# Parallel Slopes Model Implications

$$y = \beta_0 + \beta_1x + \beta_2 d + \epsilon$$
$$\hat{y} = b_0 + b_1x + b_2 d$$
- where $x$ is continuous and $d$ is a dummy variable = 0/1

---
# Parallel Slopes Model Implications

$$\hat{y} = b_0 + b_1x + b_2 d$$

- Change in $y$ due to a one-unit change in $x$ is still $b_1$

--

- In cases where $d=0$, then

$$\hat{y} = b_0 + b_1x$$

--

- In cases where $d=1$, then

$$\hat{y} = (b_0 + b_2) + b_1x$$

--

- Regression line for $d=1$ group will be above or below $d=0$ group by $b2$ units

---
# Example: Parallel Slopes

$$Salary=\beta_0+\beta_1YrsEmployed+\beta_2Sex + \epsilon$$
```{r}
sal_mod <- lm(salary ~ yrs.service + sex, data = Salaries)
```
```{r, eval=FALSE}
get_regression_table(sal_mod)
```
```{r, echo=FALSE}
get_regression_table(sal_mod) %>% 
  kable(format = 'html')
```

$$\hat{Salary}=92357+748 \times YrsEmployed+9072 \times Sex$$

---
# Interpreting results

$$\hat{Salary}=92357+748 \times YrsEmployed+9072 \times Sex$$

- What is the standard interpretation for the `YrsEmployed` coefficient?

--

- What is the standard interpretation for the `Sex` coefficient?

---
# Interpreting results

$$\hat{Salary}=92357+748 \times YrsEmployed+9072 \times Sex$$

- What is the predicted change in salary for a male professor three years later?

--

- For a female professor three years later?

--

- If slopes are parallel, then the predicted change must be the same.

---
# Interpreting results

$$\hat{Salary}=92357+748 \times YrsEmployed+9072 \times Sex$$

- What is the predicted salary for a male professor employed 20 years?

--

- For a female professor employed 20 years?

--

- Predicted value will **not** be the same because each group has a different intercept

---
class: inverse, middle, center

# Let's practice another example using a parallel slopes model.

---
class: inverse, middle, center

# We aren't required to force the slopes to be parallel. Could allow the effect of years employed to differ between male and female professors.

---
# Visualizing Interaction Model

.pull-left[
```{r, eval=FALSE}
ggplot(Salaries, 
       aes(x = yrs.service, 
           y = salary, 
           color = sex)) + #<<
  geom_point(alpha = 0.6, size = 5) +
  geom_smooth(method = 'lm', se = FALSE, size = 2) + #<<
  theme(text = element_text(size = 20))
```
]

.pull-right[
```{r, echo=FALSE, message=FALSE, out.width='99%'}
ggplot(Salaries, aes(x = yrs.service, y = salary, color = sex)) +
  geom_point(alpha = 0.6, size = 5) +
  geom_smooth(method = 'lm', se = FALSE, size = 2) +
  theme(text = element_text(size = 20))
```
]

---
# Interaction model implications

$$y = \beta_0 + \beta_1x + \beta_2d + \beta_3 xd + \epsilon$$
$$\hat{y} = b_0 + b_1x + b_2d + b_3 xd$$

--

- For the $d=0$ group:

$$\hat{y} = b_0 + b_1x$$

--

- For the $d=1$ group:

$$\hat{y} = (b_0 + b_2) + (b_1+b_3)x$$

- Intercept *and* slope *can* differ between groups


---
# Interaction

- An interaction allows the effect of one variable to differ depending on the value of another variable

$$Salary=\beta_0+\beta_1YrsEmp+\beta_2Sex + \beta_3YrsEmp*Sex + \epsilon$$

- $\beta_3$ tells us how the slopes compare
  - For Sex = 1, the marginal effect of years on salary is $\beta_1+\beta_3$
  - For Sex = 0 (female), it is $\beta_1$

- $\beta_2$ tells us how the intercepts compare
  - For Sex = 1, intercept is $\beta_0+\beta_2$
  - For Sex = 0, it is $\beta_0$

---
# Running an interaction model

```{r}
sal_mod2 <- lm(salary ~ yrs.service + sex + 
                 yrs.service*sex, #<<
               data = Salaries)
```
```{r, eval=FALSE}
get_regression_table(sal_mod2)
```
```{r, echo=FALSE}
get_regression_table(sal_mod2) %>% 
  kable(format = 'html', digits = 2)
```

---
# Interpreting an interaction model

$$\hat{Sal}=82069+1637*YrsEmp+20129*Sex - 932*YrsEmp*Sex$$

- What is the predicted change in salary for a male professor given one more year of employment?

--

- What is the predicted change in salary for a female professor given one more year of employment?

---
# Interpreting an interaction model

$$\hat{Sal}=82069+1637*YrsEmp+20129*Sex - 932*YrsEmp*Sex$$

- What is the predicted salary for a female professor employed for 10 years?

--

- What is the predicted salary for a male professor employed for 10 years?

---
class: inverse, middle, center

# Let's practice another example using interactions. 

---
class: inverse, middle, center

# Linear probability model (LPM)

---
# LPM

- Regression model with a two-level (dummy) categorical variable for the response

- For a binary outcome (did occur/did not occur), an LPM estimates the effect of our explanatory variables on the probability that the outcome occurs

$$Pr(y=1)=\beta_0+\beta_1x+\dots+\beta_kx_k+\epsilon$$

---
# Running an LPM

```{r, include=FALSE}
load("lectures_files/divorce.RData")
divorce <- rename(divorce, Age25to29 = Age)
```

```{r, echo=FALSE}
head(divorce, n=5) %>% 
  kable(format = 'html')
```

- Suppose we want to know how these variables might affect the probability of divorce among married or previously married individuals

---
# Running an LPM

```{r}
div_mod <- lm(Divorce ~ Age25to29 + Income + Children,
              data = divorce)
```

```{r, eval=FALSE}
get_regression_table(div_mod)
```

```{r, echo=FALSE}
get_regression_table(div_mod) %>% 
  kable(format = 'html')
```

---
# Interpreting an LPM

$$\hat{Pr(y=1)}=0.38+0.4*Age+0.001*Income-0.18*Children$$

- How does having a child affect the probability of divorce?

--

- All else equal, each child reduces the probability of divorce by 18 **percentage points**, on average.

--

- What is the probability of divorce for someone 27 years old making 50,000 and has 0 children?

--

```{r}
0.38 + 0.4*1 + 0.001*50 - 0.18*0
```
- The probability of divorce for someone with this profile is 83%, on average.

---
class: inverse, middle, center

# Let's practice another example with using an LPM